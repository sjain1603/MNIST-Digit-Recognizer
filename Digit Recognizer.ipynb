{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "Training_iterations = 2500\n",
    "\n",
    "Dropout = 0.5\n",
    "batch = 30\n",
    "\n",
    "Validation_size = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data(42000, 785)\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0         0         0         0   \n",
      "3       0  ...         0         0         0         0         0         0   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./input/train.csv')\n",
    "\n",
    "print('data({0[0]}, {0[1]})'.format(data.shape))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images(42000, 784)\n"
     ]
    }
   ],
   "source": [
    "images = data.iloc[:, 1:].values\n",
    "images = images.astype(np.float)\n",
    "\n",
    "\n",
    "print('images({0[0]}, {0[1]})'.format(images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAB1xJREFUeJzt3T1olfcfxuHkj1CwKFJRqVAUSjqoQ/BlbBfrUEScBSmC7WAp6i4OASmoOAipyRCXdnAoBQdfwSBR1EWqQ4hQRGoGEd8pFRsV08X/UPR8n9aTk1jv6xp785w8gXx4oD/POd2Tk5NdwLvvfzN9A8D0EDuEEDuEEDuEEDuEmDXNP8//+ofO637df/RkhxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxBihxCzZvoG6KyJiYlyf/jwYVuvf/LkyXLfunVrW6/fjsnJyZbb+vXry2v37NlT7r29vW90TzPJkx1CiB1CiB1CiB1CiB1CiB1CdFfHEx0wrT8sxfj4eMvtq6++Kq8dHh5u62c3/f10d3e39frtqO6t6b4WL15c7hcvXiz3jz76qNw77LW/nCc7hBA7hBA7hBA7hBA7hBA7hBA7hPAW1/+AX3/9tdz379/fcmv3HH0mNZ119/f3l/vOnTtbbtW/Tejq6uq6detWuQ8NDZV7X19fuc8ET3YIIXYIIXYIIXYIIXYIIXYIIXYI4Zz9LfDTTz+V+7ffflvu9+7dm8rbeWt8+OGH5f7555+X+/Lly1tuTefsTWbPnt3W9TPBkx1CiB1CiB1CiB1CiB1CiB1CiB1COGefBqOjo+X+9ddfl/vvv/9e7jP52eydNDY2Vu4HDhwo97t3707l7fzNzZs3O/baneLJDiHEDiHEDiHEDiHEDiHEDiHEDiF8P/sUmJiYKPdVq1aVe9N58kx+B/rChQvLvel93ceOHWu5LVu2rLx2cHCw3L/55ptyb+f72Xt7e8v99OnT5b5gwYJy7zDfzw7JxA4hxA4hxA4hxA4hxA4hvMV1Cjx48KDcHz9+XO7tHp21c/0nn3xS7hcuXCj3Dz744I1/9o0bN8r94MGD5d7O771kyZJyP3ToULnP8NHaG/FkhxBihxBihxBihxBihxBihxBihxDe4joNDh8+XO5NX8nc9Bbads6bjx49Wu4bNmwo96Z7GxkZabnt2rWrvPaXX34p9yYbN25suX3//ffltU1fF/2W8xZXSCZ2CCF2CCF2CCF2CCF2CCF2COGc/S3Q9FHSK1asKPd2ztnnzZtX7t999125X7p0qdx//PHHf31P//fxxx+X+/bt28u96d8vvMOcs0MysUMIsUMIsUMIsUMIsUMIsUMI5+z/AU3nxQMDA9N0J69q+vtZtGhRy2337t3ltZs3by73uXPnlnsw5+yQTOwQQuwQQuwQQuwQQuwQQuwQwjn7f8Dt27fLffHixdN0J69q+vvZsmVLy21wcLC89r333nuTW8I5O2QTO4QQO4QQO4QQO4QQO4SYNdM3QFfX6OhouZ84caLcq4+SnjNnTnnt8+fPy/3Jkyfl3uTUqVMtt/Hx8fLanp6etn42f+fJDiHEDiHEDiHEDiHEDiHEDiHEDiGcs0+B+/fvl/uOHTvK/eeffy73iYmJcl+7dm3Lbe/eveW1V65cKfemj7Fuurc7d+603H777bfyWufsU8uTHUKIHUKIHUKIHUKIHUKIHUKIHUI4Z58C58+fL/czZ86U+9OnT8t91apV5d7X19dyW7lyZXlt0379+vVybzrHr1y+fLnc161b98avzas82SGE2CGE2CGE2CGE2CGE2CGE2CGEc/Z/qPps902bNpXXNp2jr1mzptyHh4fL/f333y/3dsyfP79jr7169eqOvTav8mSHEGKHEGKHEGKHEGKHEGKHEI7e/qF9+/a13Jo+Tvmzzz4r9+PHj5d7J4/WmoyMjJT75OTkNN0J7fJkhxBihxBihxBihxBihxBihxBihxDO2V969uxZuT969Kjl1t3dXV77xRdflHvTOXrTvY2NjZV75Ycffij3s2fPlnvT7960M3082SGE2CGE2CGE2CGE2CGE2CGE2CGEc/aXXrx4Ue5//vnnG792f39/uTedZTe9X/7cuXP/+p6my5w5c1punfyYal7lyQ4hxA4hxA4hxA4hxA4hxA4hxA4hnLO/9Pz583JftmxZy+3atWvltbdu3Wprb/ps9pl8z/jQ0FC5f/rppy23np6eqb4dCp7sEELsEELsEELsEELsEELsEELsEKJ7mr9f+538Mu+rV6+W+5EjR8p9YGCg3P/4449yX7RoUcvtyy+/LK9tsm3btnJfunRpW69PR7z2H154skMIsUMIsUMIsUMIsUMIsUMIR2/w7nH0BsnEDiHEDiHEDiHEDiHEDiHEDiHEDiHEDiHEDiHEDiHEDiHEDiHEDiHEDiGm+yubZ+67hSGcJzuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuEEDuE+Au6dlYbDaK1VAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_size = images.shape[1]\n",
    "\n",
    "image_height = image_width = np.ceil(np.sqrt(image_size)).astype(np.uint8)\n",
    "\n",
    "def display(img):\n",
    "    \n",
    "    image = img.reshape(image_width, image_height)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=cm.binary)\n",
    "    \n",
    "display(images[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels[10] => 8\n",
      "(42000,)\n"
     ]
    }
   ],
   "source": [
    "labels = data['label']\n",
    "\n",
    "print ('labels[{0}] => {1}'.format(10,labels[10]))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "labels_count = np.unique(labels).shape[0]\n",
    "\n",
    "print (labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images(32000,784)\n",
      "validation_images(10000,784)\n"
     ]
    }
   ],
   "source": [
    "validation_images = images[:Validation_size]\n",
    "validation_labels = labels[:Validation_size]\n",
    "\n",
    "train_images = images[Validation_size:]\n",
    "train_labels = labels[Validation_size:]\n",
    "\n",
    "print('train_images({0[0]},{0[1]})'.format(train_images.shape))\n",
    "print('validation_images({0[0]},{0[1]})'.format(validation_images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variables(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.reshape(train_images, [-1,image_width , image_height,1])\n",
    "validation_images = tf.reshape(validation_images, [-1,image_width , image_height,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(validation_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rotation_range = 20, \n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255\n",
    ")\n",
    "\n",
    "datagen.fit(train_images)\n",
    "val_datagen.fit(validation_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 2, 2, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,185,418\n",
      "Trainable params: 1,185,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Your Code Here\n",
    "    tf.keras.layers.Conv2D(16, (3,3), strides=1, padding='same', activation = 'relu', input_shape = (28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2, padding='same'),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), strides=1, padding='same', activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2, padding='same'),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), strides=1, padding='same', activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2, padding='same'),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), strides=1, padding='same', activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2, padding='same'),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), strides=1, padding='same', activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2, padding='same'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 16s 62ms/step - loss: 0.8711 - acc: 0.6923 - val_loss: 0.1668 - val_acc: 0.9498\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 17s 69ms/step - loss: 0.2162 - acc: 0.9333 - val_loss: 0.1446 - val_acc: 0.9578\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 17s 69ms/step - loss: 0.1565 - acc: 0.9521 - val_loss: 0.1100 - val_acc: 0.9672\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 0.1258 - acc: 0.9616 - val_loss: 0.0730 - val_acc: 0.9769\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 17s 69ms/step - loss: 0.1051 - acc: 0.9680 - val_loss: 0.0844 - val_acc: 0.9732\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 0.0983 - acc: 0.9703 - val_loss: 0.0492 - val_acc: 0.9830\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 18s 70ms/step - loss: 0.0949 - acc: 0.9720 - val_loss: 0.0540 - val_acc: 0.9822\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 18s 70ms/step - loss: 0.0820 - acc: 0.9742 - val_loss: 0.0543 - val_acc: 0.9828\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 0.0759 - acc: 0.9772 - val_loss: 0.0396 - val_acc: 0.9873\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 18s 70ms/step - loss: 0.0690 - acc: 0.9787 - val_loss: 0.0492 - val_acc: 0.9851\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 18s 70ms/step - loss: 0.0652 - acc: 0.9801 - val_loss: 0.0402 - val_acc: 0.9867\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 0.0661 - acc: 0.9798 - val_loss: 0.0609 - val_acc: 0.9805\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 0.0642 - acc: 0.9803 - val_loss: 0.0343 - val_acc: 0.9896\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 18s 70ms/step - loss: 0.0620 - acc: 0.9811 - val_loss: 0.0349 - val_acc: 0.9890\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 18s 70ms/step - loss: 0.0598 - acc: 0.9816 - val_loss: 0.0392 - val_acc: 0.9880\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 0.0535 - acc: 0.9830 - val_loss: 0.0360 - val_acc: 0.9886\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 18s 70ms/step - loss: 0.0533 - acc: 0.9841 - val_loss: 0.0331 - val_acc: 0.9899\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 0.0538 - acc: 0.9836 - val_loss: 0.0369 - val_acc: 0.9888\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 0.0514 - acc: 0.9842 - val_loss: 0.0320 - val_acc: 0.9905\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 16s 64ms/step - loss: 0.0492 - acc: 0.9843 - val_loss: 0.0331 - val_acc: 0.9901\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 16s 64ms/step - loss: 0.0494 - acc: 0.9854 - val_loss: 0.0354 - val_acc: 0.9896\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 16s 64ms/step - loss: 0.0490 - acc: 0.9858 - val_loss: 0.0347 - val_acc: 0.9906\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 16s 64ms/step - loss: 0.0464 - acc: 0.9861 - val_loss: 0.0859 - val_acc: 0.9756\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 16s 64ms/step - loss: 0.0487 - acc: 0.9863 - val_loss: 0.0470 - val_acc: 0.9871\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 16s 64ms/step - loss: 0.0428 - acc: 0.9867 - val_loss: 0.0422 - val_acc: 0.9878\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 16s 64ms/step - loss: 0.0445 - acc: 0.9873 - val_loss: 0.0312 - val_acc: 0.9906\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 18s 70ms/step - loss: 0.0456 - acc: 0.9865 - val_loss: 0.0311 - val_acc: 0.9901\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 0.0441 - acc: 0.9871 - val_loss: 0.0259 - val_acc: 0.9923\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 0.0434 - acc: 0.9871 - val_loss: 0.0403 - val_acc: 0.9868\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 0.0400 - acc: 0.9881 - val_loss: 0.0385 - val_acc: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7f187bd0b8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = train_images.shape[0] // 128\n",
    "\n",
    "model.fit(\n",
    "    datagen.flow(train_images, train_labels, batch_size=128),\n",
    "    validation_data=val_datagen.flow(validation_images, validation_labels),\n",
    "    epochs=30,\n",
    "    steps_per_epoch=steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('./input/test.csv')\n",
    "\n",
    "test = tf.reshape(test, [-1,image_width , image_height,1])\n",
    "test = test / 255\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(test)\n",
    "predicts = np.argmax(predicts,axis=1)\n",
    "predicts = pd.Series(predicts, name=\"Label\")\n",
    "submission = pd.concat([pd.Series(range(1, 28001), name=\"ImageId\"), predicts], axis=1)\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
